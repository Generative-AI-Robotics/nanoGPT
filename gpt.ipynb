{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Artificial Intelligence\n",
    "\n",
    "* How to create a model that can generate images/text/audio/etc. from user prompts?\n",
    "* Unlabelled data availbale in abundance. Learn hidden sructure from the unlabelled data.\n",
    "\n",
    "In this notebook we specifically address the task of text generation.\n",
    "\n",
    "### What is text generation?\n",
    "Given a sequence, predict what is the next token in the sequence. The sequence can be a series of words or characters and the objective is to predict next word or character respectively in sequence.\n",
    "$$P(w_{t} | w_{t-1}, w_{t-2}, w_{t-3},...,w_{1})$$\n",
    "\n",
    "#### Some basic terminology:\n",
    "**Tokens/Tokenization** - Given a sequence of characters, tokenization is the process of dividing the sequence into smaller units called tokens. Tokens can be individual characters, segments of words, complete words or portions of sentences. Tokens obtained are converted into 1-hot vectors to be fed into the model.\n",
    "\n",
    "**Generative Model** - A model that learns to sample from the probability distribution to generate data that seem to be from the same probability distribution as training data.\n",
    "\n",
    "**Discriminative Model** - In contrast to generative models, discriminative models are trained to differentiate between classes or categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text:  1115393\n",
      "\n",
      "Sample text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it \n"
     ]
    }
   ],
   "source": [
    "# Read the input corpus\n",
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Length of text: \", len(text))\n",
    "print(f\"\\nSample text:\\n{text[:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "One of the easiest language model to start with is the character level model where each character is a token. It encodes minimum token level information but is easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size:  65\n"
     ]
    }
   ],
   "source": [
    "# Create characters as vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary: \", ''.join(chars))\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed characters into a model they need to converted into numbers that can be processed by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder function for idx to char and back\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43]\n",
      "Shakespeare\n"
     ]
    }
   ],
   "source": [
    "print(encode('Shakespeare'))\n",
    "print(decode(encode('Shakespeare')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to torch tensor\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Get single batch of data for training\n",
    "def get_batch(split='train', block_size=8, batch_size=4):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([14, 59, 58,  6,  1, 51, 39, 42]),\n",
       " tensor([59, 58,  6,  1, 51, 39, 42, 39]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y  = get_batch(split='train')\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "**Possible generative models**:\n",
    "1. **N-gram model** - Given n-previous tokens in the sequence, predict the next token. Most common approaches are bigram or trigram model with bayes estimation. Larger the value of **N**, more context information can be incorporated.\n",
    "2. **Recurrent neural networks** - A goto neural network achitecture for working with sequential data. Behind the scenes, just a neural network that processes each token of the sequential input one at a time. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/rnn.webp\" width=\"700\">\n",
    "</p>\n",
    "\n",
    "Condenses entire history of the sequence into a single vecctor. Theoretically RNNs can process infinite history but this is limited proctically by computational constraints and memory requirements. Even with a large enough history, RNNs struggle with long term dependencies.\n",
    "\n",
    "3. **Transformer models** - Introduced in 2017 by the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). The paper introduces an architecture that provides a differentiable lookup method for the called `Attention` that potentially solves the problem of long term dependencies by allowing the model to lookup specific information from the history as required.\n",
    "\n",
    "e.g., Prompt - Where is Eiffel Tower located? Answer - It is located in Paris. _Here `It` is related to `Eiffel Tower` and `Paris` to `Where`_.\n",
    "\n",
    "In this notebook we will start with a simple Bigram model and slowly build our way towards a Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model\n",
    "\n",
    "**Embedding layer** - Converts from an index-based representation to a vector representation i.e., each index is mapped to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 59, 58,  6,  1, 51, 39, 42])\n",
      "torch.Size([8, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "example_layer = nn.Embedding(vocab_size, vocab_size).to(device)\n",
    "print(x[0])\n",
    "print(example_layer(x[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) (4, 8, 65)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)  # calls forward function\n",
    "            logits = logits[:, -1, :] # only consider the last output\n",
    "            probs = F.softmax(logits, dim=-1) # normalize it to a probabilty distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # Sample from the distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # Add it to the generated sequence\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "xb, yb = get_batch(split='train')\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThoutMtoIQlNhv\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(encode('Thou'), dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "print(decode(m.generate(idx, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: train loss 3.7891, validation loss 3.9341\n",
      "Step 2000: train loss 3.3235, validation loss 3.7550\n",
      "Step 3000: train loss 3.1876, validation loss 3.2936\n",
      "Step 4000: train loss 2.7422, validation loss 3.2484\n",
      "Step 5000: train loss 2.7747, validation loss 2.7088\n",
      "Step 6000: train loss 2.3555, validation loss 2.7764\n",
      "Step 7000: train loss 2.4933, validation loss 2.5743\n",
      "Step 8000: train loss 2.5843, validation loss 2.6449\n",
      "Step 9000: train loss 2.6714, validation loss 2.8557\n",
      "Step 10000: train loss 2.5463, validation loss 2.1739\n",
      "Trained model validation loss: 2.6996\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=300):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split=split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = loss.mean()\n",
    "    model.train()\n",
    "    return out['train'], out['val']\n",
    "\n",
    "def train_model(model, optimizer, block_size=8, batch_size=4, train_iters=10000):\n",
    "    for step in range(train_iters):\n",
    "        xb, yb = get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if ((step + 1) % 1000 == 0):\n",
    "            train_loss, val_loss = estimate_loss(model=model)\n",
    "            print(f'Step {step + 1}: train loss {train_loss:.4f}, validation loss {val_loss:.4f}')\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "train_model(m, optimizer)\n",
    "print(f'Trained model validation loss: {estimate_loss(m)[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou poandiseron; emarlY rewillio$ou he, s:\n",
      "\n",
      "JRUCA send ayoulecont lspadoY cedshagave I whe s we,\n",
      "d he b\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(encode('Thou'), dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Models\n",
    "\n",
    "In bigram model, we are only considering the last character of the sequence to generate a new character. With the help of transformers we will enable the model to look into the entire history i.e., all the characters (limited to block size of the data) in the sequence so far.\n",
    "\n",
    "At the core of the transformer is a `single attention head` (referred to in the paper as Scaled Dot-Product Attention).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/Single-head attention.png\" width=\"200\">\n",
    "</p>\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})V$$\n",
    "\n",
    "**Q, K, V** - Query, Key and Value vectors corresponding to the vector representations for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([58,  1, 54, 50])\n",
      "input: tensor([58]), target: 1\n",
      "input: tensor([58,  1]), target: 54\n",
      "input: tensor([58,  1, 54]), target: 50\n",
      "input: tensor([58,  1, 54, 50]), target: 43\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(split='train')\n",
    "print(xb[0, :4])\n",
    "\n",
    "for b in range(1):\n",
    "    for t in range(4):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"input: {context}, target: {target}\")\n",
    "\n",
    "tril = torch.tril(torch.ones(4, 4))\n",
    "print(tril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size=32, embed_dim=32, block_size=8, dropout=0.4) -> None:\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        # Attention scores\n",
    "        weights = q @ k.transpose(-2, -1) * self.head_size**-0.5 # (B, T, head_size) @ (B, head_size, T) --> # (B, T, T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x) # (B, T, embed_dim)\n",
    "        context = weights @ v # (B, T, T) @ (B, T, embed_dim) -> (B, T, embed_dim)\n",
    "        return context\n",
    "\n",
    "    \n",
    "class LanguageModelBase(nn.Module):\n",
    "    def __init__(self, head_size=32, embed_dim=32, block_size=8, dropout=0.4) -> None:\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, embed_dim)\n",
    "        self.attention_head = SingleHeadAttention(head_size, embed_dim, block_size, dropout)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "        _, T = idx.shape\n",
    "        token_embed = self.token_embedding_table(idx) # (B, T, embed_dim)\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(T, device=device)) # (T, embed_dim)\n",
    "        x = token_embed + pos_embed # (B, T, embed_dim)\n",
    "        x = self.attention_head(x) # (B, T, embed_dim)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.553 K parameters\n",
      "Step 1000: train loss 2.5853, validation loss 2.7865\n",
      "Step 2000: train loss 2.8465, validation loss 2.7734\n",
      "Step 3000: train loss 2.3241, validation loss 2.2861\n",
      "Step 4000: train loss 2.9462, validation loss 2.6620\n",
      "Step 5000: train loss 2.3600, validation loss 2.7450\n",
      "Step 6000: train loss 2.3211, validation loss 2.7743\n",
      "Step 7000: train loss 2.6133, validation loss 2.6025\n",
      "Step 8000: train loss 2.5949, validation loss 2.6655\n",
      "Step 9000: train loss 2.3887, validation loss 2.4869\n",
      "Step 10000: train loss 2.1348, validation loss 2.3411\n",
      "Trained model validation loss: 2.3338\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModelBase()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e3, 'K parameters')\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "train_model(model=m, optimizer=optimizer)\n",
    "print(f'Trained model validation loss: {estimate_loss(m)[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou carevau esun hee :Ath he mer ths.sliincutus bthr se acci kY'si:\n",
      "Ml m nitoxc yawet betw shy s nt shr\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(encode('Thou'), dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention\n",
    "\n",
    "One possible way of understanding attention head is that it looks for specific speech information in the sequence. E.g., the head might look for relevant nouns in the sequence. The idea of multi-head attention is that each head can look for different kind of speech information, e.g., one head for relevant nouns, one for relevant verbs, one for relevant prepositions, etc.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/multihead_attention.png\" width=\"250\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=4, head_size=32, embed_dim=32, dropout=0.4, block_size=8) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(head_size, embed_dim, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim * num_heads, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads*head_size)\n",
    "        out = self.proj(out) # (B, T, embed_dim)\n",
    "        out = self.dropout(out) # (B, T, embed_dim)\n",
    "        return out\n",
    "    \n",
    "class LanguageModelMultiHead(LanguageModelBase):\n",
    "    def __init__(self, head_size=32, embed_dim=32, block_size=8, dropout=0.4, num_heads=4) -> None:\n",
    "        super().__init__(head_size, embed_dim, block_size, dropout)\n",
    "        self.attention_head = MultiHeadAttention(num_heads, head_size, embed_dim, dropout, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.897 K parameters\n",
      "Step 1000: train loss 2.7360, validation loss 2.7585\n",
      "Step 2000: train loss 2.4645, validation loss 2.2319\n",
      "Step 3000: train loss 2.4067, validation loss 2.6485\n",
      "Step 4000: train loss 2.8033, validation loss 2.4727\n",
      "Step 5000: train loss 2.5226, validation loss 2.2189\n",
      "Step 6000: train loss 2.5559, validation loss 2.5681\n",
      "Step 7000: train loss 2.3045, validation loss 2.6631\n",
      "Step 8000: train loss 2.6865, validation loss 2.5036\n",
      "Step 9000: train loss 2.4763, validation loss 2.4336\n",
      "Step 10000: train loss 2.5803, validation loss 2.2242\n",
      "Trained model validation loss: 2.2565\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModelMultiHead()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e3, 'K parameters')\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "train_model(model=m, optimizer=optimizer)\n",
    "print(f'Trained model validation loss: {estimate_loss(m)[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoug umerod g you i't clcn,\n",
      "M ilsemyofrdy\n",
      "Hhead ou\n",
      "Aranomm,\n",
      "vos,\n",
      "Amec tt br?\n",
      "obleplwardat lorig I ot\n",
      "Al\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(encode('Thou'), dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need\n",
    "\n",
    "So far we implemented a multi-head attention module. In the paper the author suggests to stack multiple such blocks, thereby increasing the depth of the network such that multiple attention blocks can interact with each other.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/transformers.png\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim=32, dropout=0.4) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=32, n_head=4, head_size=32, dropout=0.4, block_size=8) -> None:\n",
    "        super().__init__()\n",
    "        self.ma = MultiHeadAttention(n_head, head_size, embed_dim, dropout, block_size)\n",
    "        self.ffn = FeedForward(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x)) # Communication layer (B, T, embed_dim)\n",
    "        x = x + self.ffn(self.ln2(x)) # Computation layer (B, T, embed_dim)\n",
    "        return x\n",
    "    \n",
    "class LanguageModelTransformer(LanguageModelBase):\n",
    "    def __init__(self, head_size=32, embed_dim=32, block_size=8, dropout=0.4, num_head=4, num_blocks=4) -> None:\n",
    "        super().__init__(head_size, embed_dim, block_size, dropout)\n",
    "        self.attention_head = nn.Sequential(\n",
    "            *[Block(embed_dim, num_head, head_size, dropout, block_size) for _ in range(num_blocks)],\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.129 K parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: train loss 2.4669, validation loss 2.8703\n",
      "Step 2000: train loss 2.5872, validation loss 2.0732\n",
      "Step 3000: train loss 2.7851, validation loss 2.2142\n",
      "Step 4000: train loss 2.3523, validation loss 2.4505\n",
      "Step 5000: train loss 2.4850, validation loss 1.7209\n",
      "Step 6000: train loss 2.3785, validation loss 2.4120\n",
      "Step 7000: train loss 2.7865, validation loss 2.5848\n",
      "Step 8000: train loss 2.3630, validation loss 2.3989\n",
      "Step 9000: train loss 2.2790, validation loss 2.0910\n",
      "Step 10000: train loss 2.0408, validation loss 2.3849\n",
      "Trained model validation loss: 2.6399\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModelTransformer()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e3, 'K parameters')\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "train_model(model=m, optimizer=optimizer)\n",
    "print(f'Trained model validation loss: {estimate_loss(m)[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou not o I,\n",
      "Drondy Mfut IN vioverr, latt, chagutere, is foriir!\n",
      "3\n",
      "FAIS:\n",
      "Is!\n",
      "In; le to not hacho Of by \n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(encode('Thou'), dtype=torch.long).unsqueeze(dim=0).to(device)\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.110209 M parameters\n"
     ]
    }
   ],
   "source": [
    "head_size = 512\n",
    "embed_dim = 256\n",
    "block_size = 512\n",
    "dropout = 0.1\n",
    "num_head = 12\n",
    "num_blocks = 12\n",
    "batch_size = 8\n",
    "eval_iters = 300\n",
    "train_iters = 10000\n",
    "\n",
    "\n",
    "model = LanguageModelTransformer(head_size, embed_dim, block_size, dropout, num_head, num_blocks)\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "# optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# train_model(model, optimizer, block_size, batch_size, train_iters)\n",
    "# print(f'Trained model validation loss: {estimate_loss(m)[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT\n",
    "We have so far trained a GPT (Generalized Pretrained Transformer). This is the first out of three steps executed by OpenAI to train ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
