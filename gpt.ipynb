{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Artificial Intelligence\n",
    "\n",
    "* How to create a model that can generate images/text/audio/etc. from user prompts?\n",
    "* Unlabelled data availbale in abundance. Learn hidden sructure from the unlabelled data.\n",
    "\n",
    "In this notebook we specifically address the task of text generation.\n",
    "\n",
    "### What is text generation?\n",
    "Given a sequence, predict what is the next token in the sequence. The sequence can be a series of words or characters and the objective is to predict next word or character respectively in sequence.\n",
    "$$P(w_{t} | w_{t-1}, w_{t-2}, w_{t-3},...,w_{1})$$\n",
    "\n",
    "#### Some basic terminology:\n",
    "**Tokens/Tokenization** - Given a sequence of characters, tokenization is the process of dividing the sequence into smaller units called tokens. Tokens can be individual characters, segments of words, complete words or portions of sentences. Tokens obtained are converted into 1-hot vectors to be fed into the model.\n",
    "\n",
    "**Generative Model** - A model that learns to sample from the probability distribution to generate data that seem to be from the same probability distribution as training data.\n",
    "\n",
    "**Discriminative Model** - In contrast to generative models, discriminative models are trained to differentiate between classes or categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text:  1115393\n",
      "\n",
      "Sample text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it \n"
     ]
    }
   ],
   "source": [
    "# Read the input corpus\n",
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"Length of text: \", len(text))\n",
    "print(f\"\\nSample text:\\n{text[:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "One of the easiest language model to start with is the character level model where each character is a token. It encodes minimum token level information but is easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size:  65\n"
     ]
    }
   ],
   "source": [
    "# Create characters as vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary: \", ''.join(chars))\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed characters into a model they need to converted into numbers that can be processed by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder function for idx to char and back\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43]\n",
      "Shakespeare\n"
     ]
    }
   ],
   "source": [
    "print(encode('Shakespeare'))\n",
    "print(decode(encode('Shakespeare')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to torch tensor\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Get single batch of data for training\n",
    "def get_batch(split='train', block_size=8, batch_size=4):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50, 50, 11,  0, 25, 63,  1, 61]),\n",
       " tensor([50, 11,  0, 25, 63,  1, 61, 53]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y  = get_batch(split='train')\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "**Possible generative models**:\n",
    "1. **N-gram model** - Given n-previous tokens in the sequence, predict the next token. Most common approaches are bigram or trigram model with bayes estimation. Larger the value of **N**, more context information can be incorporated.\n",
    "2. **Recurrent neural networks** - A goto neural network achitecture for working with sequential data. Behind the scenes, just a neural network that processes each token of the sequential input one at a time. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/rnn.webp\" width=\"700\">\n",
    "</p>\n",
    "\n",
    "Condenses entire history of the sequence into a single vecctor. Theoretically RNNs can process infinite history but this is limited proctically by computational constraints and memory requirements. Even with a large enough history, RNNs struggle with long term dependencies.\n",
    "\n",
    "3. **Transformer models** - Introduced in 2017 by the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). The paper introduces an architecture that provides a differentiable lookup method for the called `Attention` that potentially solves the problem of long term dependencies by allowing the model to lookup specific information from the history as required.\n",
    "\n",
    "e.g., Prompt - Where is Eiffel Tower located? Answer - It is located in Paris. Here `It` is related to `Eiffel Tower`, `is` to `is` and `located` to `located`.\n",
    "\n",
    "In this notebook we will start with a simple Bigram model and slowly build our way towards a Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
